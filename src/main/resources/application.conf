hadoop {
  split.size.mb = 100  # This controls the size of input splits, affecting the number of mappers
  num.reducers = 3    # This sets the number of reducers for each job
}

tokenization {
  model-type = "TEXT_EMBEDDING_ADA_002"
  job-name = "JTokkit Tokenization"
  preprocessing-regex = "[^a-z0-9\\s]"
}

sliding-window {
  window-size = 4
  job-name = "Sliding Window for Embedding"
  input-split-delimiter = "\t"
  token-index = 1
}

embedding-job {
  window-size = 4
  embedding-size = 4
  job-name = "Embedding Generation"
  input-split-delimiter = "\t"
}

neural-network {
  activation-function = "TANH"
  output-activation-function = "IDENTITY"
  loss-function = "MSE"
}

semantic-similarity {
  job-name = "Semantic Similarity"
  input-split-delimiter = "\t"
  embedding-delimiter = "|"
  top-k = 5
  similarity-format = "%.4f"
}

statistics-collater {
  job-name = "Statistics Collater"
  input-split-delimiter = "\t"
  similarity-pair-delimiter = ","
  similarity-score-delimiter = "("
  na-value = "N/A"
}

yaml-output {
  word-key = "word"
  int-token-key = "intToken"
  frequency-key = "frequency"
  similar-tokens-key = "semanticallySimilarTokens"
}

pipeline {
  jobs = [
    {
      name = "tokenization"
      class = "TokenizationJob"
    },
    {
      name = "sliding_window"
      class = "SlidingWindowJob"
    },
    {
      name = "embedding"
      class = "EmbeddingJob"
    },
    {
      name = "semantic_similarity"
      class = "SemanticSimilarityJob"
    }
  ]
  final-job {
    name = "statistics_collated"
    num.reducers = 1
  }
}